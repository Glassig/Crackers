This research was performed using gamification by implementing two versions of a text-based game. Tests were then performed where a test group played both versions and evaluated them separately. 

\section{The Game}
The game is inspired by the text-based adventure game Zork (see section \ref{sec:zork}). It consists of a few rooms and tasks to be performed before reaching a victory scenario. Two different versions of the game were created, where one version utilizes typing to control your characterâ€™s actions and the other utilizes speech. The versions differ in environment and plot, so that a user who has played one control-scheme could still play the other without having the benefit of knowing what is required to win.

\subsection{Plot}
The user plays as a hungry pet bunny that has escaped its cage and is on the hunt for food. The goal is to solve puzzles in different rooms in order to find crackers to eat. The game is completed when three crackers are found and eaten. The speech version takes place in an apartment with the rooms kitchen, living room and bedroom. The text version takes place in a school with the rooms classroom, hallway and cafeteria.

\section{Implementation}
When implementing the different versions of the game two existing open source libraries were used for word tagging and speech recognition, namely Stanford POSTagger and Sphinx4 described in sections \ref{sec:postagger} and \ref{sec:sphinx4}. These were then linked to our own built parser, which takes two words as arguments: one verb and one noun. These words are sorted out from the user's command line using the word tagger. The parser then generates the proper response by first handling the verb and then linking the action to the given noun. Verbs handled in the parser are ``go'', ``look'', ``take'', ``eat'' and ``use''. Several synonyms to these verbs are also handled by first sending them through a custom-built synonym checker that converts them to one of the five verbs handled by the parser. If the verb is not recognized the game responds with ``Try something else''.

\subsection{Programming Language}
The choice of programming language depended on which existing libraries to be used in the game. Java was convenient to use since there were many libraries to choose from that were available in this programming language.

The development of the game was done in an integrated development environment (IDE) called Eclipse, which provide a lot of useful tools whereof some for handling dependencies for all the various libraries used.

\subsection{Stanford POSTagger} \label{sec:postagger}
The library used for tagging command words is the Stanford Part-Of-Speech Tagger. It is part of the Stanford CoreNLP, which is a suite of core NLP tools. A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word, such as noun, verb, adjective, etc. \citep{POSTagger} This library was used because it is one of the most acknowledged free part-of-speech taggers.

By tagging each word in the user's command line it was possible to sort out which command words were verbs and nouns, which were then sent to the parser. By doing this the user was able to communicate with the game using natural language. For example commands like ``take the toy located under the couch in the kitchen'' will be handled as ``take toy'', since those words are the first verb and noun pair in the command. Some commands run through the Stanford POSTagger will however be tagged incorrectly if proper grammar is not used. For example when using the command ``use key on door'' the word ``use'' is tagged as a noun instead of a verb, however if you instead use the command ``use the key on the door'' the word ``use'' is tagged as a verb. To handle this issue all nouns were run through the synonym checker as well to see if it matched any of our verbs.

\subsection{Sphinx4} \label{sec:sphinx4}
The Sphinx4 speech recognition system is the latest addition to Carnegie Mellon University's repository of the Sphinx speech recognition systems. It is universal in its acceptance of various kinds of grammars and language models, types of acoustic models and feature streams. Sphinx4 is developed entirely in the Java programming language and is widely used, which made it suitable for use in the game. \citep{Sphinx4}

Sphinx4 is used solely in the speech version of the game, where the user give commands through speech using a microphone. When a command is spoken, Sphinx4 recognizes the separate words and then converts them to text. It is then sent to the Stanford POSTagger and so on.

Which words and command structures Sphinx4 can recognize is specified in grammar files. For example specific verbs and nouns can be specified and then the command structure can be set as \textit{<verb> <noun>}, which would make Sphinx4 recognize commands like ``use key'' but not commands like ``use the small golden key''. In the game the speech command structure is set as
\\
\\
\textit{<command> = <verb> [<conjunction> <determiner>] <noun> | <cmd>.}
\\
\\
The straight line symbolizes ``or'', so either structure separated by the straight line is acceptable. The conjunctions and determiners are optional, making both commands like ``go to the kitchen'' and ``go kitchen'' recognizable. The \textit{<cmd>} contain special commands like ``quit'' and ``help''.

When implementing Sphinx4 into the game it turned out that the more recognizable words, the higher risk of Sphinx4 misinterpreting the spoken command. Although, cutting down on the amount of synonyms would make the input less natural language like. We found a balance between amount of synonyms and recognition by picking out the most relevant synonyms and removing more unlikely ones. In addition to this, separate grammar files were made for each room in the game, making it possible to limit the amount of nouns recognizable in each room. For example the word ``cat'' is recognizable in the living room but not in the kitchen or bedroom.

\section{Evaluation}
\subsection{User Testing}
Before the user played any version of the game, they filled in a form asking for some of their personal data, how well they would rate their spoken and written English and if they had any speech impediments, see appendix \ref{appA} figure \ref{fig:ud_1} and \ref{fig:ud_2}. The user then played one version of the game and afterwards filled in the ``System Usability Scale''-questionnaire presented in section \ref{usability}. They then played the other version and once again filled in the questionnaire.

Users tested both versions of the game so that each of the evaluation scores could be compared. The game version the user played first was alternated, so that about half of the users started with the speech version and half with the text version. Each game version started with an introductory text, explaining how to play the game, the basic structure of commands and that the user should try using synonyms if stuck at any point. It also explained the goal and the name of all rooms. This was the only thing the user was told before they started inputting commands. While they played they were given hints only if they were stuck at some point for quite a while. Examples of these hints are ``speak clearer'', ``try synonyms'' or ``input should be at least a verb and a noun''.

While the user was playing, the game recorded each command, how many commands were used and the total time played. When the user completed the game this data was saved to a text file that was later used for analysis.

\subsection{System Usability Scale} \label{sec:sus}
Using the System Usability Scale as described in \ref{usability}, a score for each system is calculated. However, since all the odd questions are ``positive statements'' (for example ``I think that I would like to use this system frequently'') while all the even questions are ``negative statements'' (such as ``I found the system unnecessarily complex'') in nature, they have to be converted in order to make them work together. This is done as follows: 
\begin{equation} \label{eq:convert}
	    f_{i} = 
	\begin{cases} 
	    5 - Q_{i}, & \text{when i is even}\\
	    Q_{i} - 1, & \text{when i is odd}
	\end{cases}
\end{equation}
Where \(Q_{i} \) is the answer to question numbered \textit{i}. The total score is then calculated using the formula: 
\begin{equation} \label{eq:sum}
	2.5 * \displaystyle \sum_{i=1}^{10} f_{i} 
\end{equation}
The score lies in the range 0-100. A higher score means that the system is easy to use and liked by the users, while a lower score means that it should be improved before publishing. \citep{Broo}

A research aiming to describe how the system usability score translates into an adjective rating scale of usability came to the conclusion shown in table \ref{fig:susadj}. \citep[page 118]{Aaron} This is relevant in order to draw a conclusion from the achieved usability score, whereas otherwise there would be no specific indications of what is considered a good or bad score.

\begin{table}[ht]
  \centering
  \begin{tabular}{lc}
    \toprule
    Adjective & Mean SUS Score\\
    \midrule
    Best Imaginable & 90.9\\
    Excellent & 85.5\\
    Good & 71.4\\
    OK & 50.9\\
    Poor & 35.7\\
    Awful & 20.3\\
    Worst Imaginable & 12.5\\
    \bottomrule
  \end{tabular}
  \caption{Adjective Ratings for SUS Scores}\label{fig:susadj}
\end{table}

The System Usability Scale was chosen because it is an industry standard with references in over 1300 articles and publications and is considered valid and easy to administer to participants. \citep{Gov}