\section{The Game}
The game is inspired by the previously mentioned Zork. It consists of a few rooms and tasks to be performed before reaching a victory scenario. The game differs in environments and plot between the different implementations of NLIs, which requires the user to input different commands in order to win. One version uses typing to control your characterâ€™s actions and the other uses speech. The reason we decided to create two implementations is so that a user who has played one control-scheme could still play the other without having the benefit of knowing what is required to win.

\subsection{Plot}
The user play as a bunny that has escaped its cage and is on the hunt for food. They need to eat three crackers in each game in order to ease their hunger and win the game. In the speech version the user is a house-pet and is in an apartment and has the ability to be in the kitchen, the living room and the bedroom. In the text version they are a class-pet in a school and can visit the classroom, the hallway and the cafeteria.

\section{Implementation}
When implementing the different versions of the game existing open source libraries were used for word tagging and speech recognition, which were then linked to our own built parser. The parser takes two words as arguments: one verb and one noun. These words are sorted out from the user's command line using the word tagger. The parser then generates the proper response by first handling the verb and then linking the action to the given noun. Verbs handled in the parser are ``go'', ``look'', ``take'', ``eat'' and ``use''. Several synonyms to these verbs are also handled by first sending them through a synonym checker that converts them to one of the five verbs handled by the parser. If the verb is not recognized the game responds with ``Try something else''.

\subsection{Programming Language}
The choice of programming language depended on which existing libraries to be used in the game. Java was convenient to use since both of us were comfortable using it and there were many libraries to choose from that were adjusted to work in Java.

The development of the game was done in an integrated development environment (IDE) called Eclipse, which provide a lot of useful tools whereof some for easily handling dependencies for all the various libraries used. To synchronize our coding progress with each other Git was used, which is a popular version control system.

\subsection{Stanford POSTagger}
The library which was used for tagging command words is the Stanford Part-Of-Speech Tagger. It is part of the Stanford CoreNLP, which is a suite of core NLP tools. A Part-Of-Speech Tagger (POS Tagger) is a piece of software that reads text in some language and assigns parts of speech to each word, such as noun, verb, adjective, etc. \citep{POSTagger}

By tagging each word in the user's command line it was possible to sort out which command words were verbs and nouns, the types handled in the parser. By doing this the user was able to communicate with the game using natural language. For example commands like ``take the toy located under the couch in the kitchen'' will be handled as ``take toy'', since those words are the verb and noun in the command. Some commands run through the Stanford POSTagger will however be tagged incorrectly if proper grammar is not used. For example when using the command ``use key on door'' the word ``use'' gets tagged as a noun instead of a verb, however if you instead use the command ``use the key on the door'' the word ``use'' gets tagged as a verb. To handle this issue all nouns were run through the synonym checker as well to see if it matched any of our verbs.

\subsection{Sphinx4}
The Sphinx4 speech recognition system is the latest addition to Carnegie Mellon University's repository of the Sphinx speech recognition systems. It is universal in its acceptance of various kinds of grammars and language models, types of acoustic models and feature streams. Sphinx 4 is developed entirely in the Java programming language and is widely used, which made it suitable for use in the game. \citep{Sphinx4}

Sphinx4 is used solely in the speech version of the game, where the user give commands through speech using a microphone. When a command is spoken, Sphinx4 recognizes the separate words and then converts the command to text. It is then sent to the Stanford POSTagger, etc.

Which words and command structures Sphinx4 can recognize is specified in grammar files (with extension .gram). For example specific verbs and nouns can be specified and then the command structure can be set as \textit{<verb> <noun>}, which would make Sphinx4 recognize commands like ``use key'' but not commands like ``use the small golden key''. In the game the speech command structure is set as
\\
\\
\textit{<command> = <verb> <conjunction> <determiner> <noun> | <cmd>.}
\\
\\
The straight line symbolizes ``or'', so either structure separated by the straight line is acceptable. The conjunctions and determiners are optional, making both commands like ``go to the kitchen'' and ``go kitchen'' recognizable. The \textit{<cmd>} contain special commands like ``quit'' and ``help''.

When implementing Sphinx4 into the game it turned out that the more recognizable words, the higher risk of Sphinx4 misinterpreting the spoken command. Although, cutting down on the amount of synonyms would make the input less natural language like. We found a balance between amount of synonyms and recognition by picking out the most relevant synonyms and removing more unlikely ones. In addition to this, separate grammar files were made for each room in the game, making it possible to limit the amount of nouns recognizable in each room. For example the word ``cat'' is recognizable in the kitchen but not in the kitchen or bedroom.

\section{Evaluation}
\subsection{User Testing}
Users tested both versions of the game so that each of the score given could be compared. Each game starts with an introductory text, explaining how to play the game, the basic structure of commands and that the user should try using synonyms if stuck at any point. It also explains the goal and the name of all the rooms. This is the only thing the user is told before they start inputting commands. While they play they might only be given hints if the user is stuck at some point for quite a while. Examples of these hints are ``speak clearer'', ``try synonyms'' or ``input should be at least a verb and a noun''.

Before the user plays any game, they fill in a form asking for their personal data, how well they would rate their spoken and written english and if they have any speech impediment, see appendix \ref{appA} figure \ref{fig:ud_1} and \ref{fig:ud_2}. The user then plays one version of the game and after fills in the ``System Usability Scale''-questionnaire detailed in \ref{usability}. They then plays the other version and once again fills in the questionnaire. The game the user starts to play is varied between the users, so that almost half of the testers started with speech and half with text.

While the user plays the game records each command and how many commands is used. When the user is done this data gets saved to a file that is later done for analysis.

\subsection{System Usability Scale} \label{sec:sus}
Using the System Usability Scale as described in \ref{usability}, a score for each system is calculated. However, since all the odd questions are ``positive statements'' (for example ``I think that I would like to use this system frequently'') while all the even questions are ``negative statements'' (such as ``I found the system unnecessarily complex'') in nature, they have to be converted in order to make them work together. This is done as follows: 
\begin{equation} \label{eq:convert}
	    f_{i} = 
	\begin{cases} 
	    5 - Q_{i}, & \text{when i is even}\\
	    Q_{i} - 1, & \text{when i is odd}
	\end{cases}
\end{equation}
Where \(Q_{i} \) is the answer to question numbered i. The total point is then: 
\begin{equation} \label{eq:sum}
	2.5 * \displaystyle \sum_{i=1}^{10} f_{i} 
\end{equation}
The score lies in the range 0-100. A high score means that the system is easy to use and liked by the users, while a low score means that it should be improved before publishing. \citep{Broo}
